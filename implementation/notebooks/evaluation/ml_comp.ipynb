{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # ACC: Accuracy\t-> (Number of Correct Predictions) / (Total Number of Predictions\n",
    "    # FPR: False Positive Rate\t-> Number of FP / (FP +TN)\n",
    "    # AUC: Area under the ROC curve\t-> scikit-learn\n",
    "    # Ya: Yield of actives \t-> TP/(TP+FP)\n",
    "    # EF: Enrichment Factor\t-> ((TP)/(TP+FP))((tp+fn)/(tp+tn+fp+fn))\n",
    "    # REF: Relative Enrichment Factor -> 100*tp/min(tp+fp,tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "import os \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "base_dir = Path(os.getcwd())/\"implementation\"\n",
    "result_dir = base_dir / \"data/results/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics_from_result_df(df: pd.DataFrame, name: str):\n",
    "    label = df[\"LABEL\"]\n",
    "    pred = df[\"PRED\"]\n",
    "\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    for i, v in enumerate(pred):\n",
    "        if v == 1 and label[i] == 1:\n",
    "            true_positive += 1\n",
    "        elif v == 1 and label[i] == 0:\n",
    "            false_positive += 1\n",
    "        elif v == 0 and label[i] == 0:\n",
    "            true_negative += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "\n",
    "    resultdf = pd.DataFrame()\n",
    "    resultdf[\"name\"] = [name]\n",
    "    resultdf[\"ACC\"] = [(true_positive + true_negative) / len(label)]\n",
    "    resultdf[\"FPR\"] = [(false_positive) / (false_positive + true_negative)]\n",
    "    resultdf[\"AUC\"] = roc_auc_score(label, pred)\n",
    "    resultdf[\"YA\"] = true_positive / (true_positive+false_positive)\n",
    "    resultdf[\"EF\"] = [\n",
    "        ((true_positive) / (true_positive + false_positive))\n",
    "        / ((true_positive + false_negative) / (len(label)))\n",
    "    ]\n",
    "    resultdf[\"REF\"] = [\n",
    "        (100 * true_positive)\n",
    "        / min((true_positive + false_positive), (true_positive + false_negative))\n",
    "    ]\n",
    "    return resultdf\n",
    "\n",
    "\n",
    "def print_roc_curve(df: pd.DataFrame,path:Path):\n",
    "    label = df[\"LABEL\"]\n",
    "    pred = df[\"PRED\"]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(label, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    display = RocCurveDisplay(\n",
    "        fpr=fpr,\n",
    "        tpr=tpr,\n",
    "        roc_auc=roc_auc,\n",
    "        estimator_name=\"ROC Curve\",\n",
    "    )\n",
    "    display.plot()\n",
    "    #plt.show()\n",
    "    plt.savefig(path)\n",
    "\n",
    "\n",
    "def print_conf_matrix(df: pd.DataFrame,path:Path):\n",
    "    label = df[\"LABEL\"]\n",
    "    pred = df[\"PRED\"]\n",
    "    label = [\"active\" if i == 1 else \"inactive\" for i in label]\n",
    "    pred = [\"active\" if i == 1 else \"inactive\" for i in pred]\n",
    "\n",
    "    cm = confusion_matrix(label, pred, labels=[\"active\",\"inactive\"])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"active\",\"inactive\"])\n",
    "    disp.plot()\n",
    "    plt.savefig(path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = {\n",
    "    \"ache\": pd.read_csv(result_dir / \"ACHE/baseline_rf.csv\"),\n",
    "    \"cox1\": pd.read_csv(result_dir / \"COX1/baseline_rf.csv\"),\n",
    "    \"dpp4\": pd.read_csv(result_dir / \"DPP4/baseline_rf.csv\"),\n",
    "    \"maob\": pd.read_csv(result_dir / \"MAOB/baseline_rf.csv\"),\n",
    "    \"seh\": pd.read_csv(result_dir / \"SEH/baseline_rf.csv\"),\n",
    "}\n",
    "knn = {\n",
    "    \"ache\": pd.read_csv(result_dir / \"ACHE/fe_rf_per_knn.csv\"),\n",
    "    \"cox1\": pd.read_csv(result_dir / \"COX1/fe_rf_per_knn.csv\"),\n",
    "    \"dpp4\": pd.read_csv(result_dir / \"DPP4/fe_rf_per_knn.csv\"),\n",
    "    \"maob\": pd.read_csv(result_dir / \"MAOB/fe_rf_per_knn.csv\"),\n",
    "    \"seh\": pd.read_csv(result_dir / \"SEH/fe_rf_per_knn.csv\"),\n",
    "}\n",
    "nn = {\n",
    "    \"ache\": pd.read_csv(result_dir / \"ACHE/fe_smote_nn.csv\"),\n",
    "    \"cox1\": pd.read_csv(result_dir / \"COX1/baseline_nn.csv\"),\n",
    "    \"dpp4\": pd.read_csv(result_dir / \"DPP4/baseline_nn.csv\"),\n",
    "    \"maob\": pd.read_csv(result_dir / \"MAOB/baseline_nn.csv\"),\n",
    "    \"seh\": pd.read_csv(result_dir / \"SEH/baseline_nn.csv\"),\n",
    "}\n",
    "\n",
    "ml = {\"rf\":rf,\n",
    "      \"nn\":nn,\n",
    "      \"knn\":knn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{machine learning algorithms comparison}\n",
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "Name & ACC & FPR & AUC & YA & EF & REF \\\\\n",
      "\\midrule\n",
      "rf & 0.7762 & 0.1380 & 0.6916 & 0.8276 & 2.3596 & 85.8631 \\\\\n",
      "knn & 0.7352 & 0.2292 & 0.6684 & 0.6387 & 1.7419 & 69.6539 \\\\\n",
      "nn & 0.7246 & 0.1937 & 0.6530 & 0.6215 & 1.6647 & 63.6876 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_metrics = pd.DataFrame(columns=[\"Name\",\"ACC\",\"FPR\",\"AUC\",\"YA\",\"EF\",\"REF\"])\n",
    "for name,a in ml.items():\n",
    "    r = pd.DataFrame()\n",
    "    for k, v in a.items():\n",
    "        r = pd.concat([r, calc_metrics_from_result_df(v, name=k)])\n",
    "    r = r.drop(columns=\"name\")\n",
    "    r = r.aggregate([\"mean\"])\n",
    "    row = [name]\n",
    "    row.extend(r.loc[\"mean\",:].values.flatten().tolist())\n",
    "    ml_metrics.loc[len(ml_metrics[\"Name\"])] = row\n",
    "\n",
    "ml_metrics = ml_metrics.sort_values(\"ACC\", ascending=False)\n",
    "\n",
    "print(\n",
    "    ml_metrics.to_latex(\n",
    "        index=False,\n",
    "        float_format=\"{:.4f}\".format,\n",
    "        escape=True,\n",
    "        caption=\"machine learning algorithms comparison\",\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activity_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
