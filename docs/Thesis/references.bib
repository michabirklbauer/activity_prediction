 
 @Article{Myers2001,
  author       = {Myers, S. and Baker, A.},
  date         = {2001-08},
  journaltitle = {Nature Biotechnology},
  title        = {Drug discovery--an operating model for a new era},
  doi          = {10.1038/90765},
  issn         = {1087-0156},
  language     = {eng},
  number       = {8},
  pages        = {727--730},
  volume       = {19},
  file         = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/11479559:text/html},
  keywords     = {Biotechnology, Chemistry, Drug Design, Drug Industry, Genome, Humans, Workforce},
  pmid         = {11479559},
}

 @online{WikiReliquienschrein2022,
	title={Reliquienschrein},
	url={https://de.wikipedia.org/wiki/Reliquienschrein},
	date={2022-08-29},
	urldate={2023-02-11},
	langid={ngerman}
}

 
 @Article{Wouters2020,
  author       = {Wouters, Olivier J. and McKee, Martin and Luyten, Jeroen},
  date         = {2020-03},
  journaltitle = {JAMA},
  title        = {Estimated {Research} and {Development} {Investment} {Needed} to {Bring} a {New} {Medicine} to {Market}, 2009-2018},
  doi          = {10.1001/jama.2020.1166},
  issn         = {1538-3598},
  language     = {eng},
  number       = {9},
  pages        = {844--853},
  volume       = {323},
  abstract     = {IMPORTANCE: The mean cost of developing a new drug has been the subject of debate, with recent estimates ranging from \$314 million to \$2.8 billion. OBJECTIVE: To estimate the research and development investment required to bring a new therapeutic agent to market, using publicly available data. DESIGN AND SETTING: Data were analyzed on new therapeutic agents approved by the US Food and Drug Administration (FDA) between 2009 and 2018 to estimate the research and development expenditure required to bring a new medicine to market. Data were accessed from the US Securities and Exchange Commission, Drugs@FDA database, and ClinicalTrials.gov, alongside published data on clinical trial success rates. EXPOSURES: Conduct of preclinical and clinical studies of new therapeutic agents. MAIN OUTCOMES AND MEASURES: Median and mean research and development spending on new therapeutic agents approved by the FDA, capitalized at a real cost of capital rate (the required rate of return for an investor) of 10.5\% per year, with bootstrapped CIs. All amounts were reported in 2018 US dollars. RESULTS: The FDA approved 355 new drugs and biologics over the study period. Research and development expenditures were available for 63 (18\%) products, developed by 47 different companies. After accounting for the costs of failed trials, the median capitalized research and development investment to bring a new drug to market was estimated at \$985.3 million (95\% CI, \$683.6 million-\$1228.9 million), and the mean investment was estimated at \$1335.9 million (95\% CI, \$1042.5 million-\$1637.5 million) in the base case analysis. Median estimates by therapeutic area (for areas with ≥5 drugs) ranged from \$765.9 million (95\% CI, \$323.0 million-\$1473.5 million) for nervous system agents to \$2771.6 million (95\% CI, \$2051.8 million-\$5366.2 million) for antineoplastic and immunomodulating agents. Data were mainly accessible for smaller firms, orphan drugs, products in certain therapeutic areas, first-in-class drugs, therapeutic agents that received accelerated approval, and products approved between 2014 and 2018. Results varied in sensitivity analyses using different estimates of clinical trial success rates, preclinical expenditures, and cost of capital. CONCLUSIONS AND RELEVANCE: This study provides an estimate of research and development costs for new therapeutic agents based on publicly available data. Differences from previous studies may reflect the spectrum of products analyzed, the restricted availability of data in the public domain, and differences in underlying assumptions in the cost calculations.},
  file         = {:Wouters2020 - Estimated Research and Development Investment Needed to Bring a New Medicine to Market, 2009 2018.html:URL},
  keywords     = {Costs and Cost Analysis, Drug Costs, Drug Development, Drug Industry, Pharmaceutical Research, United States, United States Food and Drug Administration},
  pmcid        = {PMC7054832},
  pmid         = {32125404},
}

 
@Article{Song2009,
  author       = {Song, Chun Meng and Lim, Shen Jean and Tong, Joo Chuan},
  date         = {2009-09},
  journaltitle = {Briefings in Bioinformatics},
  title        = {Recent advances in computer-aided drug design},
  doi          = {10.1093/bib/bbp023},
  issn         = {1467-5463},
  number       = {5},
  pages        = {579--591},
  url          = {https://doi.org/10.1093/bib/bbp023},
  urldate      = {2024-02-29},
  volume       = {10},
  abstract     = {Modern drug discovery is characterized by the production of vast quantities of compounds and the need to examine these huge libraries in short periods of time. The need to store, manage and analyze these rapidly increasing resources has given rise to the field known as computer-aided drug design (CADD). CADD represents computational methods and resources that are used to facilitate the design and discovery of new therapeutic solutions. Digital repositories, containing detailed information on drugs and other useful compounds, are goldmines for the study of chemical reactions capabilities. Design libraries, with the potential to generate molecular variants in their entirety, allow the selection and sampling of chemical compounds with diverse characteristics. Fold recognition, for studying sequence-structure homology between protein sequences and structures, are helpful for inferring binding sites and molecular functions. Virtual screening, the in silico analog of high-throughput screening, offers great promise for systematic evaluation of huge chemical libraries to identify potential lead candidates that can be synthesized and tested. In this article, we present an overview of the most important data sources and computational methods for the discovery of new molecular entities. The workflow of the entire virtual screening campaign is discussed, from data collection through to post-screening analysis.},
  file         = {:Song2009 - Recent Advances in Computer Aided Drug Design.pdf:PDF},
}

 

 
@Article{DiMasi2003,
  author       = {DiMasi, Joseph A. and Hansen, Ronald W. and Grabowski, Henry G.},
  date         = {2003-03},
  journaltitle = {Journal of Health Economics},
  title        = {The price of innovation: new estimates of drug development costs},
  doi          = {10.1016/S0167-6296(02)00126-1},
  issn         = {0167-6296},
  language     = {eng},
  number       = {2},
  pages        = {151--185},
  volume       = {22},
  abstract     = {The research and development costs of 68 randomly selected new drugs were obtained from a survey of 10 pharmaceutical firms. These data were used to estimate the average pre-tax cost of new drug development. The costs of compounds abandoned during testing were linked to the costs of compounds that obtained marketing approval. The estimated average out-of-pocket cost per new drug is 403 million US dollars (2000 dollars). Capitalizing out-of-pocket costs to the point of marketing approval at a real discount rate of 11\% yields a total pre-approval cost estimate of 802 million US dollars (2000 dollars). When compared to the results of an earlier study with a similar methodology, total capitalized costs were shown to have increased at an annual rate of 7.4\% above general price inflation.},
  file         = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/12606142:text/html},
  keywords     = {Capital Expenditures, Costs and Cost Analysis, Data Collection, Drug Approval, Drug Evaluation, Drug Evaluation, Preclinical, Drug Industry, Drugs, Investigational, Humans, Inflation, Economic, Organizational Innovation, Research Support as Topic, United States},
  pmid         = {12606142},
  shorttitle   = {The price of innovation},
}

 
@Article{Mayr2008,
  author       = {Mayr, Lorenz M. and Fuerst, Peter},
  date         = {2008-07},
  journaltitle = {SLAS Discovery},
  title        = {The {Future} of {High}-{Throughput} {Screening}},
  doi          = {10.1177/1087057108319644},
  issn         = {2472-5552},
  number       = {6},
  pages        = {443--448},
  url          = {https://www.sciencedirect.com/science/article/pii/S2472555222082260},
  urldate      = {2024-02-29},
  volume       = {13},
  abstract     = {High-throughput screening (HTS) is a well-established process in lead discovery for pharma and biotech companies and is now also being set up for basic and applied research in academia and some research hospitals. Since its first advent in the early to mid-1990s, the field of HTS has seen not only a continuous change in technology and processes but also an adaptation to various needs in lead discovery. HTS has now evolved into a quite mature discipline of modern drug discovery. Whereas in previous years, much emphasis has been put toward a steady increase in capacity (“quantitative increase”) via various strategies in the fields of automation and miniaturization, the past years have seen a steady shift toward higher content and quality (“quality increase”) for these biological test systems. Today, many experts in the field see HTS at the crossroads with the need to decide either toward further increase in throughput or more focus toward relevance of biological data. In this article, the authors describe the development of HTS over the past decade and point out their own ideas for future directions of HTS in biomedical research. They predict that the trend toward further miniaturization will slow down with the implementation of 384-well, 1536-well, and 384 low-volume-well plates. The authors predict that, ultimately, each hit-finding strategy will be much more project related, tailor-made, and better integrated into the broader drug discovery efforts.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S2472555222082260/pdf?md5=2a40ac08b6e393e047e6064efdd88669&pid=1-s2.0-S2472555222082260-main.pdf&isDTMRedir=Y:application/pdf},
  keywords     = {high-throughput screening, lead finding, drug discovery, miniaturization, automation},
}

 
@Article{Gimeno2019,
  author       = {Gimeno, Aleix and Ojeda-Montes, María José and Tomás-Hernández, Sarah and Cereto-Massagué, Adrià and Beltrán-Debón, Raúl and Mulero, Miquel and Pujadas, Gerard and Garcia-Vallvé, Santiago},
  date         = {2019-03},
  journaltitle = {International Journal of Molecular Sciences},
  title        = {The {Light} and {Dark} {Sides} of {Virtual} {Screening}: {What} {Is} {There} to {Know}?},
  doi          = {10.3390/ijms20061375},
  issn         = {1422-0067},
  number       = {6},
  pages        = {1375},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6470506/},
  urldate      = {2024-02-29},
  volume       = {20},
  abstract     = {Virtual screening consists of using computational tools to predict potentially bioactive compounds from files containing large libraries of small molecules. Virtual screening is becoming increasingly popular in the field of drug discovery as in silico techniques are continuously being developed, improved, and made available. As most of these techniques are easy to use, both private and public organizations apply virtual screening methodologies to save resources in the laboratory. However, it is often the case that the techniques implemented in virtual screening workflows are restricted to those that the research team knows. Moreover, although the software is often easy to use, each methodology has a series of drawbacks that should be avoided so that false results or artifacts are not produced. Here, we review the most common methodologies used in virtual screening workflows in order to both introduce the inexperienced researcher to new methodologies and advise the experienced researcher on how to prevent common mistakes and the improper usage of virtual screening methodologies.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC6470506/:text/html;PubMed Central Full Text PDF:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC6470506/pdf/ijms-20-01375.pdf:application/pdf},
  pmcid        = {PMC6470506},
  pmid         = {30893780},
  shorttitle   = {The {Light} and {Dark} {Sides} of {Virtual} {Screening}},
}

 
@Article{Lavecchia2013,
  author       = {Lavecchia, A. and Di Giovanni, C.},
  date         = {2013},
  journaltitle = {Current Medicinal Chemistry},
  title        = {Virtual screening strategies in drug discovery: a critical review},
  doi          = {10.2174/09298673113209990001},
  issn         = {1875-533X},
  language     = {eng},
  number       = {23},
  pages        = {2839--2860},
  volume       = {20},
  abstract     = {Virtual screening (VS) is a powerful technique for identifying hit molecules as starting points for medicinal chemistry. The number of methods and softwares which use the ligand and target-based VS approaches is increasing at a rapid pace. What, however, are the real advantages and disadvantages of the VS technology and how applicable is it to drug discovery projects? This review provides a comprehensive appraisal of several VS approaches currently available. In the first part of this work, an overview of the recent progress and advances in both ligand-based VS (LBVS) and structure-based VS (SBVS) strategies highlighting current problems and limitations will be provided. Special emphasis will be given to in silico chemogenomics approaches which utilize annotated ligand-target as well as protein-ligand interaction databases and which could predict or reveal promiscuous binding and polypharmacology, the knowledge of which would help medicinal chemists to design more potent clinical candidates with fewer side effects. In the second part, recent case studies (all published in the last two years) will be discussed where the VS technology has been applied successfully. A critical analysis of these case studies provides a good platform in order to estimate the applicability of various VS strategies in the new lead identification and optimization.},
  file         = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/23651302:text/html},
  keywords     = {Drug Discovery, Drug Evaluation, Preclinical, Genomics, Humans, Ligands, Proteins},
  pmid         = {23651302},
  shorttitle   = {Virtual screening strategies in drug discovery},
}

 
@Article{Pagadala2017,
  author       = {Pagadala, Nataraj S. and Syed, Khajamohiddin and Tuszynski, Jack},
  date         = {2017-01},
  journaltitle = {Biophysical Reviews},
  title        = {Software for molecular docking: a review},
  doi          = {10.1007/s12551-016-0247-1},
  issn         = {1867-2450},
  number       = {2},
  pages        = {91--102},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5425816/},
  urldate      = {2024-02-29},
  volume       = {9},
  abstract     = {Molecular docking methodology explores the behavior of small molecules in the binding site of a target protein. As more protein structures are determined experimentally using X-ray crystallography or nuclear magnetic resonance (NMR) spectroscopy, molecular docking is increasingly used as a tool in drug discovery. Docking against homology-modeled targets also becomes possible for proteins whose structures are not known. With the docking strategies, the druggability of the compounds and their specificity against a particular target can be calculated for further lead optimization processes. Molecular docking programs perform a search algorithm in which the conformation of the ligand is evaluated recursively until the convergence to the minimum energy is reached. Finally, an affinity scoring function, ΔG [U total in kcal/mol], is employed to rank the candidate poses as the sum of the electrostatic and van der Waals energies. The driving forces for these specific interactions in biological systems aim toward complementarities between the shape and electrostatics of the binding site surfaces and the ligand or substrate.},
  file         = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC5425816/:text/html;PubMed Central Full Text PDF:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC5425816/pdf/12551_2016_Article_247.pdf:application/pdf},
  pmcid        = {PMC5425816},
  pmid         = {28510083},
  shorttitle   = {Software for molecular docking},
}

 
@Article{Li2019,
  author       = {Li, Jin and Fu, Ailing and Zhang, Le},
  date         = {2019-06},
  journaltitle = {Interdisciplinary Sciences: Computational Life Sciences},
  title        = {An {Overview} of {Scoring} {Functions} {Used} for {Protein}–{Ligand} {Interactions} in {Molecular} {Docking}},
  doi          = {10.1007/s12539-019-00327-w},
  issn         = {1867-1462},
  language     = {en},
  number       = {2},
  pages        = {320--328},
  url          = {https://doi.org/10.1007/s12539-019-00327-w},
  urldate      = {2024-02-29},
  volume       = {11},
  abstract     = {Currently, molecular docking is becoming a key tool in drug discovery and molecular modeling applications. The reliability of molecular docking depends on the accuracy of the adopted scoring function, which can guide and determine the ligand poses when thousands of possible poses of ligand are generated. The scoring function can be used to determine the binding mode and site of a ligand, predict binding affinity and identify the potential drug leads for a given protein target. Despite intensive research over the years, accurate and rapid prediction of protein–ligand interactions is still a challenge in molecular docking. For this reason, this study reviews four basic types of scoring functions, physics-based, empirical, knowledge-based, and machine learning-based scoring functions, based on an up-to-date classification scheme. We not only discuss the foundations of the four types scoring functions, suitable application areas and shortcomings, but also discuss challenges and potential future study directions.},
  file         = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2Fs12539-019-00327-w.pdf:application/pdf},
  keywords     = {Molecular docking, Scoring function, Ligand pose, Binding affinity, Protein–ligand interaction},
}

@MastersThesis{Birklbauer2021,
  author      = {Micha Johannes Birklbauer},
  date        = {2021-08-31},
  institution = {FH Hagenberg},
  title       = {Automatic identification of important interactionsand interaction-frequency-based scoring inprotein-ligand complexes},
}

 
@Article{Bonvin2006,
  author       = {Bonvin, Alexandre MJJ},
  date         = {2006-04},
  journaltitle = {Current Opinion in Structural Biology},
  title        = {Flexible protein–protein docking},
  doi          = {10.1016/j.sbi.2006.02.002},
  issn         = {0959-440X},
  number       = {2},
  pages        = {194--200},
  series       = {Theory and simulation/{Macromolecular} assemblages},
  url          = {https://www.sciencedirect.com/science/article/pii/S0959440X06000315},
  urldate      = {2024-02-29},
  volume       = {16},
  abstract     = {Predicting the structure of protein–protein complexes using docking approaches is a difficult problem whose major challenges include identifying correct solutions, and properly dealing with molecular flexibility and conformational changes. Flexibility can be addressed at several levels: implicitly, by smoothing the protein surfaces or allowing some degree of interpenetration (soft docking) or by performing multiple docking runs from various conformations (cross or ensemble docking); or explicitly, by allowing sidechain and/or backbone flexibility. Although significant improvements have been achieved in the modeling of sidechains, methods for the explicit inclusion of backbone flexibility in docking are still being developed. A few novel approaches have emerged involving collective degrees of motion, multicopy representations and multibody docking, which should allow larger conformational changes to be modeled.},
  file         = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0959440X06000315/pdfft?md5=6e9d43b3a78412b370b49f454b2120a7&pid=1-s2.0-S0959440X06000315-main.pdf&isDTMRedir=Y:application/pdf},
}

 
@Article{Morris2022,
  author       = {Morris, Connor and Stern, Jacob and Stark, Brenden and Christopherson, Max and Della Corte, Dennis},
  date         = {2022-11},
  journaltitle = {Journal of chemical information and modeling},
  title        = {{MILCDock}: {Machine} {Learning} {Enhanced} {Consensus} {Docking} for {Virtual} {Screening} in {Drug} {Discovery}},
  doi          = {10.1021/acs.jcim.2c00705},
  volume       = {62},
  abstract     = {Molecular docking tools are regularly used to computationally identify new molecules in virtual screening for drug discovery. However, docking tools suffer from inaccurate scoring functions with widely varying performance on different proteins. To enable more accurate ranking of active over inactive ligands in virtual screening, we created a machine learning consensus docking tool, MILCDock, that uses predictions from five traditional molecular docking tools to predict the probability a ligand binds to a protein. MILCDock was trained and tested on data from both the DUD-E and LIT-PCBA docking datasets and shows improved performance over traditional molecular docking tools and other consensus docking methods on the DUD-E dataset. LIT-PCBA targets proved to be difficult for all methods tested. We also find that DUD-E data, although biased, can be effective in training machine learning tools if care is taken to avoid DUD-E's biases during training.},
  file         = {ResearchGate Link:https\://www.researchgate.net/publication/365205442_MILCDock_Machine_Learning_Enhanced_Consensus_Docking_for_Virtual_Screening_in_Drug_Discovery:},
  shorttitle   = {{MILCDock}},
}

 
@Article{Hossin2015,
  author       = {Hossin, Mohammad and M.N, Sulaiman},
  date         = {2015-03},
  journaltitle = {International Journal of Data Mining \& Knowledge Management Process},
  title        = {A {Review} on {Evaluation} {Metrics} for {Data} {Classification} {Evaluations}},
  doi          = {10.5121/ijdkp.2015.5201},
  pages        = {01--11},
  volume       = {5},
  abstract     = {Evaluation metric plays a critical role in achieving the optimal classifier during the classification training. Thus, a selection of suitable evaluation metric is an important key for discriminating and obtaining the optimal classifier. This paper systematically reviewed the related evaluation metrics that are specifically designed as a discriminator for optimizing generative classifier. Generally, many generative classifiers employ accuracy as a measure to discriminate the optimal solution during the classification training. However, the accuracy has several weaknesses which are less distinctiveness, less discriminability, less informativeness and bias to majority class data. This paper also briefly discusses other metrics that are specifically designed for discriminating the optimal solution. The shortcomings of these alternative metrics are also discussed. Finally, this paper suggests five important aspects that must be taken into consideration in constructing a new discriminator metric.},
  file         = {Full Text PDF:https\://www.researchgate.net/profile/Mohammad-Hossin/publication/275224157_A_Review_on_Evaluation_Metrics_for_Data_Classification_Evaluations/links/57b2c95008ae95f9d8f6154f/A-Review-on-Evaluation-Metrics-for-Data-Classification-Evaluations.pdf:application/pdf;ResearchGate Link:https\://www.researchgate.net/publication/275224157_A_Review_on_Evaluation_Metrics_for_Data_Classification_Evaluations:},
}

 
@Article{Lopes2017,
  author       = {Lopes, Julio Cesar Dias and dos Santos, Fábio Mendes and Martins-José, Andrelly and Augustyns, Koen and De Winter, Hans},
  date         = {2017-02},
  journaltitle = {Journal of Cheminformatics},
  title        = {The power metric: a new statistically robust enrichment-type metric for virtual screening applications with early recovery capability},
  doi          = {10.1186/s13321-016-0189-4},
  issn         = {1758-2946},
  number       = {1},
  pages        = {7},
  url          = {https://doi.org/10.1186/s13321-016-0189-4},
  urldate      = {2024-04-20},
  volume       = {9},
  abstract     = {A new metric for the evaluation of model performance in the field of virtual screening and quantitative structure–activity relationship applications is described. This metric has been termed the power metric and is defined as the fraction of the true positive rate divided by the sum of the true positive and false positive rates, for a given cutoff threshold. The performance of this metric is compared with alternative metrics such as the enrichment factor, the relative enrichment factor, the receiver operating curve enrichment factor, the correct classification rate, Matthews correlation coefficient and Cohen’s kappa coefficient. The performance of this new metric is found to be quite robust with respect to variations in the applied cutoff threshold and ratio of the number of active compounds to the total number of compounds, and at the same time being sensitive to variations in model quality. It possesses the correct characteristics for its application in early-recognition virtual screening problems.},
  file         = {Full Text PDF:Lopes2017 - The Power Metric_ a New Statistically Robust Enrichment Type Metric for Virtual Screening Applications with Early Recovery Capability.pdf:PDF:https\://jcheminf.biomedcentral.com/counter/pdf/10.1186/s13321-016-0189-4},
  keywords     = {Power metric (PM), Virtual screening, Metric, Model performance, Enrichment factor, Area under the curve (AUC), Receiver operating curve enrichment factor (ROCE), Correct classification rate (CCR), Matthews correlation coefficient (MCC), Cohen’s kappa coefficient (CKC), Relative enrichment factor (REF)},
  shorttitle   = {The power metric},
}

 
@Article{Giordano2022,
  author       = {Giordano, Deborah and Biancaniello, Carmen and Argenio, Maria Antonia and Facchiano, Angelo},
  date         = {2022-05},
  journaltitle = {Pharmaceuticals},
  title        = {Drug {Design} by {Pharmacophore} and {Virtual} {Screening} {Approach}},
  doi          = {10.3390/ph15050646},
  issn         = {1424-8247},
  number       = {5},
  pages        = {646},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9145410/},
  urldate      = {2024-04-20},
  volume       = {15},
  abstract     = {Computer-aided drug discovery techniques reduce the time and the costs needed to develop novel drugs. Their relevance becomes more and more evident with the needs due to health emergencies as well as to the diffusion of personalized medicine. Pharmacophore approaches represent one of the most interesting tools developed, by defining the molecular functional features needed for the binding of a molecule to a given receptor, and then directing the virtual screening of large collections of compounds for the selection of optimal candidates. Computational tools to create the pharmacophore model and to perform virtual screening are available and generated successful studies. This article describes the procedure of pharmacophore modelling followed by virtual screening, the most used software, possible limitations of the approach, and some applications reported in the literature.},
  file         = {PubMed Central Link:Giordano2022 - Drug Design by Pharmacophore and Virtual Screening Approach.html:URL:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC9145410/;PubMed Central Full Text PDF:Giordano2022 - Drug Design by Pharmacophore and Virtual Screening Approach.pdf:PDF:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC9145410/pdf/pharmaceuticals-15-00646.pdf},
  pmcid        = {PMC9145410},
  pmid         = {35631472},
}

@Article{Molinaro2005,
  author       = {Molinaro, Annette M. and Simon, Richard and Pfeiffer, Ruth M.},
  date         = {2005-05},
  journaltitle = {Bioinformatics},
  title        = {{Prediction error estimation: a comparison of resampling methods}},
  doi          = {10.1093/bioinformatics/bti499},
  eprint       = {https://academic.oup.com/bioinformatics/article-pdf/21/15/3301/50340684/bioinformatics\_21\_15\_3301.pdf},
  issn         = {1367-4803},
  number       = {15},
  pages        = {3301-3307},
  url          = {https://doi.org/10.1093/bioinformatics/bti499},
  volume       = {21},
  abstract     = {{Motivation: In genomic studies, thousands of features are collected on relatively few samples. One of the goals of these studies is to build classifiers to predict the outcome of future observations. There are three inherent steps to this process: feature selection, model selection and prediction assessment. With a focus on prediction assessment, we compare several methods for estimating the ‘true’ prediction error of a prediction model in the presence of feature selection.Results: For small studies where features are selected from thousands of candidates, the resubstitution and simple split-sample estimates are seriously biased. In these small samples, leave-one-out cross-validation (LOOCV), 10-fold cross-validation (CV) and the .632+ bootstrap have the smallest bias for diagonal discriminant analysis, nearest neighbor and classification trees. LOOCV and 10-fold CV have the smallest bias for linear discriminant analysis. Additionally, LOOCV, 5- and 10-fold CV, and the .632+ bootstrap have the lowest mean square error. The .632+ bootstrap is quite biased in small sample sizes with strong signal-to-noise ratios. Differences in performance among resampling methods are reduced as the number of specimens available increase.Contact:  annette.molinaro@yale.eduSupplementary Information: A complete compilation of results and R code for simulations and analyses are available in Molinaro et al. (2005) (http://linus.nci.nih.gov/brb/TechReport.htm).}},
}

 
@Article{Xu2018,
  author       = {Xu, Yun and Goodacre, Royston},
  date         = {2018-07},
  journaltitle = {Journal of Analysis and Testing},
  title        = {On {Splitting} {Training} and {Validation} {Set}: {A} {Comparative} {Study} of {Cross}-{Validation}, {Bootstrap} and {Systematic} {Sampling} for {Estimating} the {Generalization} {Performance} of {Supervised} {Learning}},
  doi          = {10.1007/s41664-018-0068-2},
  issn         = {2509-4696},
  language     = {en},
  number       = {3},
  pages        = {249--262},
  url          = {https://doi.org/10.1007/s41664-018-0068-2},
  urldate      = {2024-04-21},
  volume       = {2},
  abstract     = {Model validation is the most important part of building a supervised model. For building a model with good generalization performance one must have a sensible data splitting strategy, and this is crucial for model validation. In this study, we conducted a comparative study on various reported data splitting methods. The MixSim model was employed to generate nine simulated datasets with different probabilities of mis-classification and variable sample sizes. Then partial least squares for discriminant analysis and support vector machines for classification were applied to these datasets. Data splitting methods tested included variants of cross-validation, bootstrapping, bootstrapped Latin partition, Kennard-Stone algorithm (K-S) and sample set partitioning based on joint X–Y distances algorithm (SPXY). These methods were employed to split the data into training and validation sets. The estimated generalization performances from the validation sets were then compared with the ones obtained from the blind test sets which were generated from the same distribution but were unseen by the training/validation procedure used in model construction. The results showed that the size of the data is the deciding factor for the qualities of the generalization performance estimated from the validation set. We found that there was a significant gap between the performance estimated from the validation set and the one from the test set for the all the data splitting methods employed on small datasets. Such disparity decreased when more samples were available for training/validation, and this is because the models were then moving towards approximations of the central limit theory for the simulated datasets used. We also found that having too many or too few samples in the training set had a negative effect on the estimated model performance, suggesting that it is necessary to have a good balance between the sizes of training set and validation set to have a reliable estimation of model performance. We also found that systematic sampling method such as K-S and SPXY generally had very poor estimation of the model performance, most likely due to the fact that they are designed to take the most representative samples first and thus left a rather poorly representative sample set for model performance estimation.},
  file         = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2Fs41664-018-0068-2.pdf:application/pdf},
  keywords     = {Cross-validation, Bootstrapping, Bootstrapped Latin partition, Kennard-Stone algorithm, SPXY, Model selection, Model validation, Partial least squares for discriminant analysis, Support vector machines},
  shorttitle   = {On {Splitting} {Training} and {Validation} {Set}},
}

@Comment{jabref-meta: databaseType:biblatex;}
